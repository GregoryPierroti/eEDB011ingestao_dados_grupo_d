# Pipeline Cloud Streaming ETL
## AWS Lambda + S3 + SQS (RAW â†’ Trusted â†’ Delivery)

Uma pipeline de processamento de dados em tempo real implementada com Terraform, seguindo as melhores prÃ¡ticas de arquitetura de dados moderna.

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAW       â”‚    â”‚    SQS      â”‚    â”‚   LAMBDA    â”‚    â”‚  TRUSTED    â”‚
â”‚   Bucket    â”œâ”€â”€â”€â–ºâ”‚   Queue     â”œâ”€â”€â”€â–ºâ”‚    ETL      â”œâ”€â”€â”€â–ºâ”‚   Bucket    â”‚
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                                                â–¼
                                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                      â”‚  DELIVERY   â”‚
                                                      â”‚   Bucket    â”‚
                                                      â”‚             â”‚
                                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

1. **RAW Layer**: Dados brutos sÃ£o carregados no bucket S3 RAW
2. **Event Trigger**: Upload dispara notificaÃ§Ã£o para SQS
3. **Lambda Processing**: FunÃ§Ã£o Lambda processa dados atravÃ©s das camadas
4. **TRUSTED Layer**: Dados validados, limpos e enriquecidos
5. **DELIVERY Layer**: Dados agregados e prontos para consumo

## ğŸš€ Deploy RÃ¡pido

```bash
# 1. Clone e configure
git clone <repository>
cd 07_ETL_Lambda_SQS

# 2. Configure AWS credentials
aws configure

# 3. Execute o deployment
chmod +x deploy.sh
./deploy.sh
```

## ğŸ“‹ PrÃ©-requisitos

- **Terraform** >= 1.0
- **AWS CLI** configurado
- **Python 3.11+**
- **pip3**
- PermissÃµes AWS apropriadas

## ğŸ”§ ConfiguraÃ§Ã£o Manual

### 1. InicializaÃ§Ã£o do Terraform

```bash
terraform init
```

### 2. CriaÃ§Ã£o do Pacote Lambda

```bash
# Instalar dependÃªncias
pip3 install -r requirements.txt -t ./lambda_package/
cp lambda_function.py ./lambda_package/

# Criar ZIP
cd lambda_package && zip -r ../lambda_function.zip . && cd ..
```

### 3. Deployment da Infraestrutura

```bash
# Planejar
terraform plan

# Aplicar
terraform apply
```

## ğŸ“ Estrutura do Projeto

```
07_ETL_Lambda_SQS/
â”œâ”€â”€ main.tf                     # ConfiguraÃ§Ã£o principal
â”œâ”€â”€ variables.tf                # VariÃ¡veis globais
â”œâ”€â”€ outputs.tf                  # Outputs do Terraform
â”œâ”€â”€ terraform.tfvars            # Valores das variÃ¡veis
â”œâ”€â”€ lambda_function.py          # CÃ³digo ETL Python
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ deploy.sh                   # Script de deployment
â”œâ”€â”€ README.md                   # Esta documentaÃ§Ã£o
â””â”€â”€ modules/
    â”œâ”€â”€ s3/                     # MÃ³dulo S3
    â”‚   â”œâ”€â”€ main.tf
    â”‚   â”œâ”€â”€ variables.tf
    â”‚   â””â”€â”€ outputs.tf
    â”œâ”€â”€ sqs/                    # MÃ³dulo SQS
    â”‚   â”œâ”€â”€ main.tf
    â”‚   â”œâ”€â”€ variables.tf
    â”‚   â””â”€â”€ outputs.tf
    â””â”€â”€ lambda/                 # MÃ³dulo Lambda
        â”œâ”€â”€ main.tf
        â”œâ”€â”€ variables.tf
        â””â”€â”€ outputs.tf
```

## ğŸ—„ï¸ Camadas de Dados

### RAW Layer
- **PropÃ³sito**: Armazenamento de dados brutos
- **Formato**: Qualquer (CSV, JSON, Excel)
- **RetenÃ§Ã£o**: 7 anos (com lifecycle policy)
- **Particionamento**: Por data de upload

### TRUSTED Layer
- **PropÃ³sito**: Dados validados e limpos
- **Formato**: Parquet (otimizado)
- **TransformaÃ§Ãµes**:
  - RemoÃ§Ã£o de duplicatas
  - Tratamento de valores nulos
  - ValidaÃ§Ã£o de tipos de dados
  - AplicaÃ§Ã£o de regras de negÃ³cio
  - CÃ¡lculo de score de qualidade
- **Particionamento**: `year=YYYY/month=MM/day=DD/`

### DELIVERY Layer
- **PropÃ³sito**: Dados prontos para anÃ¡lise
- **Formato**: Parquet particionado
- **ConteÃºdo**:
  - Registros detalhados
  - AgregaÃ§Ãµes diÃ¡rias
  - MÃ©tricas de qualidade
  - Metadados de processamento
- **Particionamento**: `record_type={DETAIL|DAILY_SUMMARY}/year=YYYY/month=MM/day=DD/`

## âš¡ Processamento ETL

### Funcionalidades Implementadas

#### Data Quality
- âœ… DetecÃ§Ã£o e remoÃ§Ã£o de duplicatas
- âœ… Tratamento inteligente de valores nulos
- âœ… ValidaÃ§Ã£o de tipos de dados
- âœ… DetecÃ§Ã£o de outliers
- âœ… Score de qualidade por registro

#### Enriquecimento
- âœ… CategorizaÃ§Ã£o automÃ¡tica de valores
- âœ… CÃ¡lculos derivados
- âœ… Metadados de processamento
- âœ… Timestamp de processamento

#### AgregaÃ§Ãµes
- âœ… SumarizaÃ§Ã£o diÃ¡ria
- âœ… MÃ©tricas estatÃ­sticas (count, sum, mean)
- âœ… Particionamento por tipo de registro

### Regras de NegÃ³cio

```python
# Exemplo de regras implementadas
- ValidaÃ§Ã£o de valores negativos em campos de valor
- DetecÃ§Ã£o de datas futuras
- CategorizaÃ§Ã£o automÃ¡tica por faixas de valor
- CÃ¡lculo de score de qualidade baseado em completude
```

## ğŸ” Monitoramento

### CloudWatch Logs
```bash
# Acompanhar logs em tempo real
aws logs tail /aws/lambda/etl-processor --follow

# Buscar logs por perÃ­odo
aws logs filter-log-events \
  --log-group-name /aws/lambda/etl-processor \
  --start-time $(date -d "1 hour ago" +%s)000
```

### MÃ©tricas Importantes
- **InvocaÃ§Ãµes**: NÃºmero de execuÃ§Ãµes da Lambda
- **DuraÃ§Ã£o**: Tempo de processamento
- **Erros**: Falhas na execuÃ§Ã£o
- **SQS Messages**: Mensagens na fila
- **S3 Objects**: Objetos processados

### Alertas Recomendados
```bash
# Criar alerta para erros Lambda
aws cloudwatch put-metric-alarm \
  --alarm-name "ETL-Lambda-Errors" \
  --alarm-description "ETL Lambda function errors" \
  --metric-name Errors \
  --namespace AWS/Lambda \
  --statistic Sum \
  --period 300 \
  --evaluation-periods 1 \
  --threshold 1 \
  --comparison-operator GreaterThanOrEqualToThreshold
```

## ğŸ§ª Testes

### Teste com Dados de Exemplo

```bash
# Criar arquivo de teste
cat > sample_data.csv << EOF
id,name,amount,date,category
1,Transaction A,150.50,2024-01-15,Sales
2,Transaction B,75.25,2024-01-16,Marketing
3,Transaction C,300.00,2024-01-17,Sales
EOF

# Upload para bucket RAW
aws s3 cp sample_data.csv s3://SEU_RAW_BUCKET/
```

### ValidaÃ§Ã£o do Processamento

```bash
# Verificar buckets
aws s3 ls s3://SEU_TRUSTED_BUCKET/ --recursive
aws s3 ls s3://SEU_DELIVERY_BUCKET/ --recursive

# Baixar arquivo processado
aws s3 cp s3://SEU_DELIVERY_BUCKET/record_type=DETAIL/year=2024/month=01/day=15/delivery_detail_sample_data_20240115_120000.parquet ./
```

## ğŸ“Š Consultas de Exemplo

### AWS Athena Setup

```sql
-- Criar database
CREATE DATABASE etl_pipeline;

-- Tabela TRUSTED
CREATE EXTERNAL TABLE etl_pipeline.trusted_data (
    id bigint,
    name string,
    amount double,
    date string,
    category string,
    processed_timestamp timestamp,
    source_file string,
    processing_layer string,
    data_quality_score double
)
PARTITIONED BY (
    year string,
    month string,
    day string
)
STORED AS PARQUET
LOCATION 's3://SEU_TRUSTED_BUCKET/';

-- Tabela DELIVERY
CREATE EXTERNAL TABLE etl_pipeline.delivery_data (
    id bigint,
    name string,
    amount double,
    amount_category string,
    record_type string,
    delivery_timestamp timestamp,
    processing_layer string,
    pipeline_version string
)
PARTITIONED BY (
    record_type string,
    year string,
    month string,
    day string
)
STORED AS PARQUET
LOCATION 's3://SEU_DELIVERY_BUCKET/';
```

### Consultas Ãšteis

```sql
-- Registros processados hoje
SELECT record_type, COUNT(*) as total_records
FROM etl_pipeline.delivery_data
WHERE year = '2024' AND month = '01' AND day = '15'
GROUP BY record_type;

-- Qualidade dos dados
SELECT 
    AVG(data_quality_score) as avg_quality,
    MIN(data_quality_score) as min_quality,
    MAX(data_quality_score) as max_quality
FROM etl_pipeline.trusted_data
WHERE year = '2024' AND month = '01';

-- Top categorias por valor
SELECT 
    category,
    SUM(amount) as total_amount,
    COUNT(*) as transaction_count
FROM etl_pipeline.trusted_data
WHERE year = '2024'
GROUP BY category
ORDER BY total_amount DESC;
```

## ğŸ”’ SeguranÃ§a

### IAM Permissions
- Lambda tem acesso mÃ­nimo necessÃ¡rio aos buckets S3
- SQS com encryption habilitada
- S3 buckets com public access bloqueado
- Versionamento habilitado em todos os buckets

### Encryption
- S3: Server-side encryption (AES256)
- SQS: KMS encryption
- Lambda: Encryption at rest

## ğŸ’° OtimizaÃ§Ã£o de Custos

### Lifecycle Policies
```hcl
# RAW bucket
- 30 dias â†’ Standard-IA
- 90 dias â†’ Glacier
- 2555 dias â†’ Deletion (7 anos)

# TRUSTED bucket
- 60 dias â†’ Standard-IA
- 180 dias â†’ Glacier
```

### ConfiguraÃ§Ãµes Lambda
- **Memory**: 512MB (ajustÃ¡vel conforme necessidade)
- **Timeout**: 5 minutos
- **Reserved Concurrency**: NÃ£o configurada (uso sob demanda)

## ğŸš¨ Troubleshooting

### Problemas Comuns

#### Lambda Timeout
```bash
# Aumentar timeout
aws lambda update-function-configuration \
  --function-name etl-processor \
  --timeout 600  # 10 minutos
```

#### SQS Messages em DLQ
```bash
# Verificar DLQ
aws sqs get-queue-attributes \
  --queue-url DEAD_LETTER_QUEUE_URL \
  --attribute-names ApproximateNumberOfMessages

# Reprocessar mensagens da DLQ
aws sqs receive-message --queue-url DEAD_LETTER_QUEUE_URL
```

#### Erro de PermissÃ£o S3
```bash
# Verificar IAM role da Lambda
aws iam get-role --role-name LAMBDA_ROLE_NAME
aws iam list-attached-role-policies --role-name LAMBDA_ROLE_NAME
```

### Logs de Debug

```python
# Adicionar debug no lambda_function.py
import logging
logging.getLogger().setLevel(logging.DEBUG)

# Logs detalhados aparecerÃ£o no CloudWatch
```

## ğŸ“ˆ Escalabilidade

### ConfiguraÃ§Ãµes para Alto Volume

```bash
# Aumentar batch size SQSâ†’Lambda
aws lambda update-event-source-mapping \
  --uuid EVENT_SOURCE_MAPPING_UUID \
  --batch-size 50

# Configurar reserved concurrency
aws lambda put-reserved-concurrency \
  --function-name etl-processor \
  --reserved-concurrency-configuration 100
```

### Particionamento AvanÃ§ado

```python
# Exemplo de particionamento por categoria
partition_path = f"category={row['category']}/year={timestamp.year}/month={timestamp.month:02d}/day={timestamp.day:02d}"
```

## ğŸ”„ Backup e Recovery

### Versionamento S3
- Todos os buckets tÃªm versionamento habilitado
- Permite recovery de dados sobrescritos
- Cross-region replication pode ser configurada

### Backup de ConfiguraÃ§Ãµes
```bash
# Export Terraform state
terraform show -json > infrastructure-backup.json

# Backup de cÃ³digo Lambda
aws lambda get-function \
  --function-name etl-processor \
  --query 'Code.Location' \
  --output text | xargs curl -o lambda-backup.zip
```

## ğŸŒ Ambientes

### ConfiguraÃ§Ã£o Multi-Ambiente

```bash
# Desenvolvimento
terraform workspace new dev
terraform apply -var-file="dev.tfvars"

# ProduÃ§Ã£o
terraform workspace new prod
terraform apply -var-file="prod.tfvars"
```

### VariÃ¡veis por Ambiente

```hcl
# dev.tfvars
aws_region = "us-east-1"
bucket_prefix = "etl-pipeline-dev"
lambda_function_name = "etl-processor-dev"

# prod.tfvars
aws_region = "us-east-1"
bucket_prefix = "etl-pipeline-prod"
lambda_function_name = "etl-processor-prod"
```

## ğŸ“š Recursos Adicionais

### DocumentaÃ§Ã£o AWS
- [Lambda Best Practices](https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html)
- [S3 Performance](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)
- [SQS Best Practices](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html)

### Terraform Resources
- [AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [Terraform Best Practices](https://www.terraform-best-practices.com/)

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes.

## âœ¨ PrÃ³ximos Passos

- [ ] Implementar CDC (Change Data Capture)
- [ ] Adicionar suporte a streaming com Kinesis
- [ ] IntegraÃ§Ã£o com Apache Airflow
- [ ] Dashboard de monitoramento com QuickSight
- [ ] Implementar data lineage tracking
- [ ] Adicionar testes automatizados
- [ ] CI/CD pipeline com GitHub Actions

---

**Desenvolvido com â¤ï¸ para processamento de dados em escala na AWS**
